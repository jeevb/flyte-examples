iamRoleARN: ""
inputs:
    config: '{"learning_rate":0.0003,"eval_steps":500,"instruction_key":"instruction","data_path":"yahma/alpaca-cleaned","debug_train_data_size":1024,"wandb_run_name":"","lora_r":8,"batch_size":32,"lora_target_modules":["q_proj","k_proj","v_proj"],"output_key":"output","resume_from_checkpoint":null,"output_dir":"./output","lora_dropout":0.05,"cutoff_len":512,"debug_mode":false,"lora_alpha":16,"wandb_log_model":"","train_on_inputs":true,"save_steps":50,"max_steps":300,"micro_batch_size":8,"wandb_project":"unionai-llm-fine-tuning","lr_scheduler_type":"cosine","num_epochs":1,"weight_decay":0.02,"publish_config":{"repo_id":"unionai/Llama-2-13b-LoRA-alpaca-cleaned","readme":"# Llama-2-13b fine-tuned on LoRA alpaca-cleaned","model_card":{"language":["en"],"license":"apache-2.0","tags":["pytorch","causal-lm","llama2","fine-tuning","alpaca"]}},"base_model":"meta-llama/Llama-2-13b-hf","input_key":"input","add_eos_token":true,"wandb_watch":"","device_map":"auto","warmup_ratio":0.03,"val_set_size":0,"group_by_length":true}'
envs: {}
kubeServiceAcct: ""
targetDomain: "development"
targetProject: "platform9"
version: 94D7X7lmepKWN3CjFz69XQ==
workflow: fine_tuning.llm_fine_tuning_qlora.fine_tune
